{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/a06/rmori/spillover/data/1982/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1988/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1966/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1954/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1975/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1972/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1978/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1959/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1985/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1961/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1965/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1981/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1957/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1976/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1971/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1962/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1968/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1986/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1958/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1960/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1984/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1979/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1973/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1974/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1989/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1967/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1983/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1969/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1987/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1963/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1970/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1977/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1980/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1964/tone-a01.csv\n",
      "/work/a06/rmori/spillover/data/1956/tone-a01.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# æ¤œç´¢å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "data_dir = \"/work/a06/rmori/spillover/data\"\n",
    "\n",
    "# æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ ¼ç´\n",
    "matching_paths = []\n",
    "\n",
    "# å†å¸°çš„ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if \"tone-a01\" in file:\n",
    "            full_path = os.path.join(root, file)\n",
    "            matching_paths.append(full_path)\n",
    "\n",
    "# çµæœã‚’è¡¨ç¤º\n",
    "for path in matching_paths:\n",
    "    print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = \"ç”Ÿæ´»ä¿è­·\"\n",
    "word2 = \"å°¾é“å¸‚\"\n",
    "word3 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# ç©ºç™½ï¼ˆå…¨è§’ãƒ»åŠè§’ï¼‰ã‚’å‰Šé™¤ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚‚ç”¨æ„\n",
    "def remove_spaces(text):\n",
    "    return re.sub(r'[\\s\\u3000]', '', text)  # \\s: åŠè§’ç©ºç™½ã€\\u3000: å…¨è§’ç©ºç™½\n",
    "\n",
    "# æ¤œç´¢å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "data_dir = \"/work/a06/rmori/spillover/data\"\n",
    "\n",
    "# æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ ¼ç´\n",
    "matching_paths = []\n",
    "\n",
    "# å†å¸°çš„ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    stripped = remove_spaces(content)\n",
    "                    if (word1 in stripped) and (word2 in stripped) and (word3 in stripped):\n",
    "                        matching_paths.append(full_path)\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    with open(full_path, 'r', encoding='shift_jis') as f:\n",
    "                        content = f.read()\n",
    "                        stripped = remove_spaces(content)\n",
    "                        if (word1 in stripped) and (word2 in stripped) and (word3 in stripped):\n",
    "                            matching_paths.append(full_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ èª­è¾¼å¤±æ•—: {full_path} ({e})\")\n",
    "\n",
    "# çµæœã‚’è¡¨ç¤º\n",
    "for path in matching_paths:\n",
    "    print(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# å¯¾è±¡ã¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆåˆ—åå€™è£œï¼‰\n",
    "keywords = [\"è€•åœ°é¢ç©\", \"è£½é€ æ¥­å‡ºè·é¡\", \"è£½é€ æ¥­äº‹æ¥­æ‰€\", \"å•†åº—æ•°\"]  # å¿…è¦ã«å¿œã˜ã¦å¢—ã‚„ã™\n",
    "\n",
    "# åˆä½µã—ãŸæ—§å¸‚ç”ºæ‘\n",
    "merged_towns = [\n",
    "    \"æˆ¸å‚æ‘\", \"ä¸­å±±æ‘\", \"äº•å£æ‘\", \"æ²¼ç”°ç”º\", \"å®‰ä½ç”º\", \"å¯éƒ¨ç”º\", \"ç¥‡åœ’ç”º\", \"å®‰å¤å¸‚ç”º\", \"ä½æ±ç”º\",\n",
    "    \"é«˜é™½ç”º\", \"ç€¬é‡å·ç”º\", \"ç™½æœ¨ç”º\", \"ç†Šé‡è·¡æ‘\", \"å®‰èŠ¸ç”º\", \"çŸ¢é‡ç”º\", \"èˆ¹è¶Šç”º\", \"äº”æ—¥å¸‚\"\n",
    "]\n",
    "\n",
    "# ç©ºç™½é™¤å»ç”¨é–¢æ•°\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    ç©ºç™½ã€å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã€æ”¹è¡Œãªã©ã‚’ã™ã¹ã¦å‰Šé™¤ã—ã¦æ¯”è¼ƒç”¨æ–‡å­—åˆ—ã‚’è¿”ã™\n",
    "    \"\"\"\n",
    "    return re.sub(r'[\\s\\u3000\\r\\n]+', '', str(text))\n",
    "\n",
    "# å„é …ç›®ã”ã¨ã«çµæœã‚’è¨˜éŒ²\n",
    "results_by_keyword = {kw: [] for kw in keywords}\n",
    "\n",
    "for path in sorted(matching_paths):\n",
    "    match = re.search(r\"(\\d{4})\", path)\n",
    "    if not match:\n",
    "        continue\n",
    "    year = int(match.group(1))\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, header=None, encoding='shift_jis', errors='ignore')\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ èª­è¾¼å¤±æ•—: {path} ({e})\")\n",
    "        continue\n",
    "\n",
    "    # åºƒå³¶å¸‚ã®è¡Œç•ªå·ã‚’æ¢ã™\n",
    "    hiroshima_row = None\n",
    "    for r in range(df.shape[0]):\n",
    "        for c in range(df.shape[1]):\n",
    "            val = df.iat[r, c]\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            if \"åºƒå³¶å¸‚\" in normalize_text(val):\n",
    "                hiroshima_row = r\n",
    "                break\n",
    "        if hiroshima_row is not None:\n",
    "            break\n",
    "    if hiroshima_row is None:\n",
    "        print(f\"âš ï¸ åºƒå³¶å¸‚ãŒè¦‹ã¤ã‹ã‚‰ãªã„: {path}\")\n",
    "        continue\n",
    "\n",
    "    # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾ã—ã¦å‡¦ç†\n",
    "    for keyword in keywords:\n",
    "        # åºƒå³¶å¸‚ã‚ˆã‚Šä¸Šã§æœ€å¾Œã«è¦‹ã¤ã‹ã£ãŸè©²å½“åˆ—ã‚’æ¢ã™\n",
    "        keyword_col = None\n",
    "        for r in range(hiroshima_row):\n",
    "            for c in range(df.shape[1]):\n",
    "                val = df.iat[r, c]\n",
    "                if pd.isna(val):\n",
    "                    continue\n",
    "                if keyword in normalize_text(val):\n",
    "                    keyword_col = c  # æœ€å¾Œã®ã‚‚ã®ã‚’ä¿æŒ\n",
    "        if keyword_col is None:\n",
    "            print(f\"âš ï¸ {keyword} ãŒè¦‹ã¤ã‹ã‚‰ãªã„: {path}\")\n",
    "            continue\n",
    "\n",
    "        # åºƒå³¶å¸‚ã®å€¤ã‚’å–å¾—\n",
    "        hiroshima_val = pd.to_numeric(df.iat[hiroshima_row, keyword_col], errors='coerce')\n",
    "        if pd.isna(hiroshima_val):\n",
    "            hiroshima_val = 0\n",
    "\n",
    "        # åˆä½µæ—§å¸‚ç”ºæ‘ã®å€¤ã‚’åŠ ç®—\n",
    "        for r in range(df.shape[0]):\n",
    "            found = False\n",
    "            for c in range(df.shape[1]):\n",
    "                val = df.iat[r, c]\n",
    "                if pd.isna(val):\n",
    "                    continue\n",
    "                if any(town in normalize_text(val) for town in merged_towns):\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                add_val = pd.to_numeric(df.iat[r, keyword_col], errors='coerce')\n",
    "                if not pd.isna(add_val):\n",
    "                    hiroshima_val += add_val\n",
    "\n",
    "        results_by_keyword[keyword].append((year, hiroshima_val))\n",
    "\n",
    "# âœ… å‡ºåŠ›ï¼šDataFrameåŒ–ã—ã¦è¡¨ç¤º\n",
    "for keyword, data in results_by_keyword.items():\n",
    "    df_result = pd.DataFrame(data, columns=[\"year\", keyword])\n",
    "    df_result.set_index(\"year\", inplace=True)\n",
    "    print(f\"\\nğŸ“Š {keyword}ï¼ˆåºƒå³¶å¸‚ï¼‹æ—§å¸‚ç”ºæ‘ï¼‰:\")\n",
    "    print(df_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "river",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
